"""
Collecteur de donn√©es optimis√© - Version am√©lior√©e de votre code
Maximise le volume de donn√©es avec vos feeds.json existants
"""

import os
import json
from datetime import datetime, timedelta
import pandas as pd
import feedparser
from bs4 import BeautifulSoup
import requests
import tweepy
import praw
from dotenv import load_dotenv
import time
import hashlib
from urllib.parse import urljoin
import logging
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

# --------------------------
# Configuration des dossiers
# --------------------------
RAW_DIR = "data/raw"
for subdir in ['rss', 'twitter', 'reddit', 'scraping', 'newsapi', 'combined']:
    os.makedirs(f"{RAW_DIR}/{subdir}", exist_ok=True)

# --------------------------
# Utilitaires
# --------------------------
def generate_hash(text: str) -> str:
    """G√©n√®re un hash unique pour d√©tecter les doublons"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def save_data(data: List[Dict], filename: str, format='both'):
    """Sauvegarde les donn√©es en CSV et JSON"""
    if not data:
        logger.warning(f"Aucune donn√©e √† sauvegarder pour {filename}")
        return
    
    df = pd.DataFrame(data)
    
    if format in ['csv', 'both']:
        df.to_csv(f"{filename}.csv", index=False, encoding='utf-8')
        logger.info(f"‚úì Sauvegard√©: {filename}.csv ({len(data)} entr√©es)")
    
    if format in ['json', 'both']:
        df.to_json(f"{filename}.json", orient="records", force_ascii=False, indent=2)
        logger.info(f"‚úì Sauvegard√©: {filename}.json ({len(data)} entr√©es)")

# --------------------------
# 1- RSS - OPTIMIS√â avec votre feeds.json
# --------------------------
def parse_single_feed(source: str, url: str) -> List[Dict]:
    """Parse un seul feed RSS (pour parall√©lisation)"""
    articles = []
    try:
        feed = feedparser.parse(url)
        
        for entry in feed.entries:
            # Extraction robuste du contenu
            summary = ""
            if hasattr(entry, 'summary'):
                summary = BeautifulSoup(entry.summary, "html.parser").get_text()
            elif hasattr(entry, 'description'):
                summary = BeautifulSoup(entry.description, "html.parser").get_text()
            elif hasattr(entry, 'content'):
                summary = BeautifulSoup(str(entry.content), "html.parser").get_text()
            
            # Extraction de la date
            published = ""
            if hasattr(entry, 'published'):
                published = entry.published
            elif hasattr(entry, 'updated'):
                published = entry.updated
            elif hasattr(entry, 'pubDate'):
                published = entry.pubDate
            
            # Extraction du titre
            title = entry.title if hasattr(entry, 'title') else ""
            link = entry.link if hasattr(entry, 'link') else ""
            
            article = {
                "source_type": "rss",
                "source": source,
                "title": title,
                "link": link,
                "published": published,
                "summary": summary[:500],  # Limiter la taille
                "content_hash": generate_hash(title + link),
                "retrieved_date": datetime.now().isoformat()
            }
            articles.append(article)
        
        logger.info(f"  ‚úì {source}: {len(articles)} articles")
        return articles
        
    except Exception as e:
        logger.error(f"  ‚úó {source}: {e}")
        return []

def collect_rss():
    """Collecte RSS avec votre fichier feeds.json - VERSION PARALL√àLE"""
    
    # Charger feeds.json
    try:
        with open("feeds.json", "r", encoding='utf-8') as f:
            rss_feeds = json.load(f)
        logger.info(f"[RSS] {len(rss_feeds)} feeds charg√©s depuis feeds.json")
    except FileNotFoundError:
        logger.error("[RSS] ‚ö†Ô∏è Fichier feeds.json introuvable!")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"[RSS] ‚ö†Ô∏è Erreur lecture feeds.json: {e}")
        return []
    
    all_articles = []
    
    # Traitement parall√®le pour aller plus vite
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(parse_single_feed, source, url): source 
                   for source, url in rss_feeds.items()}
        
        for future in as_completed(futures):
            articles = future.result()
            all_articles.extend(articles)
    
    today = datetime.today().strftime("%Y-%m-%d")
    save_data(all_articles, f"{RAW_DIR}/rss/articles_{today}")
    logger.info(f"[RSS] ‚úÖ Total: {len(all_articles)} articles")
    return all_articles

# --------------------------
# 2- NewsAPI - OPTIMIS√â (strat√©gie multi-pays)
# --------------------------
def collect_newsapi():
    """Collecte NewsAPI avec strat√©gie HEADLINES (meilleure pour version gratuite)"""
    
    api_key = os.getenv("NEWS_API_KEY")
    if not api_key or api_key == "votre_cle_newsapi_ici":
        logger.warning("[NewsAPI] ‚ö†Ô∏è Cl√© API manquante ou invalide")
        return []
    
    articles = []
    
    # STRAT√âGIE OPTIMALE: Top headlines par pays/cat√©gorie
    # Version gratuite: acc√®s complet aux headlines
    countries = ['us', 'gb', 'fr', 'de', 'ca', 'au', 'it', 'es', 'jp', 'kr']
    categories = ['general', 'business', 'technology', 'science', 'health', 'entertainment', 'sports']
    
    total_requests = len(countries) * len(categories)
    logger.info(f"[NewsAPI] {total_requests} requ√™tes pr√©vues (limite: 100/jour)")
    
    request_count = 0
    
    for country in countries:
        for category in categories:
            request_count += 1
            
            # Arr√™ter si on approche de la limite (garder une marge)
            if request_count >= 95:
                logger.warning("[NewsAPI] ‚ö†Ô∏è Limite de requ√™tes approch√©e, arr√™t")
                break
            
            logger.info(f"[NewsAPI] ({request_count}/{total_requests}) {country}/{category}")
            
            url = "https://newsapi.org/v2/top-headlines"
            params = {
                'apiKey': api_key,
                'country': country,
                'category': category,
                'pageSize': 100  # Maximum
            }
            
            try:
                response = requests.get(url, params=params, timeout=10)
                
                if response.status_code == 426:
                    logger.error("[NewsAPI] ‚ö†Ô∏è Upgrade requis (plan gratuit limit√©)")
                    break
                
                response.raise_for_status()
                data = response.json()
                
                if data['status'] == 'ok':
                    for art in data.get("articles", []):
                        articles.append({
                            "source_type": "newsapi",
                            "source": art.get("source", {}).get("name", "Unknown"),
                            "country": country,
                            "category": category,
                            "title": art.get("title"),
                            "description": art.get("description"),
                            "url": art.get("url"),
                            "publishedAt": art.get("publishedAt"),
                            "content": art.get("content"),
                            "urlToImage": art.get("urlToImage"),
                            "content_hash": generate_hash((art.get("title") or "") + (art.get("url") or "")),
                            "retrieved_date": datetime.now().isoformat()
                        })
                    logger.info(f"  ‚úì {len(data.get('articles', []))} articles")
                else:
                    logger.error(f"  ‚úó {data.get('message', 'Erreur inconnue')}")
                    if "apiKey" in data.get('message', ''):
                        break
                
                time.sleep(1)  # Rate limiting crucial: 1 requ√™te/seconde
                
            except requests.exceptions.RequestException as e:
                logger.error(f"  ‚úó Erreur requ√™te: {e}")
                time.sleep(2)
    
    today = datetime.today().strftime("%Y-%m-%d")
    save_data(articles, f"{RAW_DIR}/newsapi/articles_{today}")
    logger.info(f"[NewsAPI] ‚úÖ Total: {len(articles)} articles")
    return articles

# --------------------------
# 3- Twitter 
# --------------------------
def collect_twitter():
    """Collecte Twitter avec queries optimis√©es pour volume maximal"""
    
    bearer_token = os.getenv("TWITTER_BEARER_TOKEN")
    if not bearer_token or bearer_token == "AAAAAAAAAAAAAAAAAAAAAA7H4wEAAAAAqtd%2B0vwOm6aaSN8SHEKFDmAm%2Bk4%3D9rynbpBWvMsKJsSi5W6EcxNHkiZmFRteKEmukX312rtnlPxrq1":
        logger.warning("[Twitter] ‚ö†Ô∏è Bearer token manquant ou invalide")
        return []
    
    try:
        client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
        # Test de connexion
        client.get_users_me()
        logger.info("[Twitter] ‚úì Connexion √©tablie")
    except Exception as e:
        logger.error(f"[Twitter] ‚úó Erreur connexion: {e}")
        return []
    
    # Queries optimis√©es pour volume et pertinence
    queries = [
        # Actualit√©s g√©n√©rales
        "breaking news", "world news", "latest news",
        # Tech & Innovation
        "AI", "artificial intelligence", "ChatGPT", "machine learning", 
        "blockchain", "crypto", "tech news", "innovation",
        # Business & Finance
        "stocks", "market", "economy", "business", "startup",
        # Science & Environment
        "climate", "science", "space", "research",
        # Sp√©cifique fran√ßais/Maroc
        "actualit√©", "politique", "√©conomie", "Maroc",
    ]
    
    tweets_data = []
    request_count = 0
    max_requests = 450  # Limite API: 450 requ√™tes / 15 min
    
    for query in queries:
        request_count += 1
        
        if request_count >= max_requests:
            logger.warning("[Twitter] ‚ö†Ô∏è Limite de requ√™tes atteinte")
            break
        
        logger.info(f"[Twitter] ({request_count}/{len(queries)}) '{query}'")
        
        try:
            response = client.search_recent_tweets(
                query=f"{query} -is:retweet -is:reply lang:en OR lang:fr",
                max_results=100,  
                tweet_fields=["created_at", "text", "author_id", "lang", "public_metrics"],
                expansions=["author_id"]
            )
            
            if response.data:
                for tweet in response.data:
                    tweets_data.append({
                        "source_type": "twitter",
                        "query": query,
                        "text": tweet.text,
                        "created_at": tweet.created_at.isoformat() if tweet.created_at else "",
                        "author_id": str(tweet.author_id),
                        "language": tweet.lang if hasattr(tweet, 'lang') else "",
                        "retweet_count": tweet.public_metrics.get('retweet_count', 0) if hasattr(tweet, 'public_metrics') else 0,
                        "like_count": tweet.public_metrics.get('like_count', 0) if hasattr(tweet, 'public_metrics') else 0,
                        "content_hash": generate_hash(tweet.text),
                        "retrieved_date": datetime.now().isoformat()
                    })
                
                logger.info(f"  ‚úì {len(response.data)} tweets")
            else:
                logger.info(f"  ‚ö†Ô∏è 0 tweets trouv√©s")
            
        except tweepy.TooManyRequests:
            logger.warning("  ‚ö†Ô∏è Rate limit atteint, pause de 15 minutes...")
            time.sleep(900)  # 15 minutes
        except tweepy.Forbidden as e:
            logger.error(f"  ‚úó Acc√®s interdit: {e}")
            break
        except Exception as e:
            logger.error(f"  ‚úó Erreur: {e}")
        
        time.sleep(1)  # Rate limiting entre requ√™tes
    
    today = datetime.today().strftime("%Y-%m-%d")
    save_data(tweets_data, f"{RAW_DIR}/twitter/tweets_{today}")
    logger.info(f"[Twitter] ‚úÖ Total: {len(tweets_data)} tweets")
    return tweets_data

# --------------------------
# 4Ô∏è‚É£ Reddit - IMPL√âMENTATION COMPL√àTE
# --------------------------
def collect_reddit():
    """Collecte Reddit avec subreddits populaires"""
    
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "NewsCollector/1.0")
    
    if not (client_id and client_secret) or client_id == "votre_client_id_ici":
        logger.warning("[Reddit] ‚ö†Ô∏è Identifiants manquants ou invalides")
        return []
    
    try:
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        # Test de connexion
        reddit.user.me()
        logger.info("[Reddit] ‚úì Connexion √©tablie")
    except Exception as e:
        logger.error(f"[Reddit] ‚úó Erreur connexion: {e}")
        return []
    
    # Subreddits pertinents pour actualit√©s
    subreddits = [
        # Actualit√©s
        "worldnews", "news", "inthenews", "UpliftingNews",
        # Tech
        "technology", "Futurology", "artificial", "MachineLearning",
        # Science
        "science", "space", "environment", "climate",
        # Business
        "business", "economics", "cryptocurrency", "stocks",
        # R√©gional
        "france", "europe", "Morocco", "Africa"
    ]
    
    posts_data = []
    
    for subreddit_name in subreddits:
        logger.info(f"[Reddit] r/{subreddit_name}")
        
        try:
            subreddit = reddit.subreddit(subreddit_name)
            
            # Collecter hot + new pour maximiser le volume
            collected = 0
            for post in subreddit.hot(limit=50):
                posts_data.append({
                    "source_type": "reddit",
                    "subreddit": subreddit_name,
                    "post_type": "hot",
                    "title": post.title,
                    "selftext": post.selftext[:500] if post.selftext else "",
                    "url": post.url,
                    "score": post.score,
                    "num_comments": post.num_comments,
                    "created_utc": datetime.fromtimestamp(post.created_utc).isoformat(),
                    "author": str(post.author),
                    "content_hash": generate_hash(post.title + post.url),
                    "retrieved_date": datetime.now().isoformat()
                })
                collected += 1
            
            for post in subreddit.new(limit=50):
                posts_data.append({
                    "source_type": "reddit",
                    "subreddit": subreddit_name,
                    "post_type": "new",
                    "title": post.title,
                    "selftext": post.selftext[:500] if post.selftext else "",
                    "url": post.url,
                    "score": post.score,
                    "num_comments": post.num_comments,
                    "created_utc": datetime.fromtimestamp(post.created_utc).isoformat(),
                    "author": str(post.author),
                    "content_hash": generate_hash(post.title + post.url),
                    "retrieved_date": datetime.now().isoformat()
                })
                collected += 1
            
            logger.info(f"  ‚úì {collected} posts")
            time.sleep(1)  # Rate limiting
            
        except Exception as e:
            logger.error(f"  ‚úó Erreur: {e}")
    
    today = datetime.today().strftime("%Y-%m-%d")
    save_data(posts_data, f"{RAW_DIR}/reddit/posts_{today}")
    logger.info(f"[Reddit] ‚úÖ Total: {len(posts_data)} posts")
    return posts_data

# --------------------------
# 5Ô∏è‚É£ Scraping - VERSION SIMPLIFI√âE ET ROBUSTE
# --------------------------
def collect_scraping():
    """Scraping simplifi√© avec s√©lecteurs g√©n√©riques robustes"""
    
    # Headers pour √©viter les blocages
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }
    
    # Sites accessibles avec s√©lecteurs simples
    sites = {
        "BBC": "https://www.bbc.com/news",
        "Reuters": "https://www.reuters.com/world/",
        "Guardian": "https://www.theguardian.com/international",
        "AlJazeera": "https://www.aljazeera.com",
        "DW": "https://www.dw.com/en/top-stories/s-9097",
        "France24": "https://www.france24.com/en/",
        "APNews": "https://apnews.com/",
        "Euronews": "https://www.euronews.com/news",
    }
    
    all_articles = []
    
    for source, url in sites.items():
        logger.info(f"[Scraping] {source}")
        
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            
            soup = BeautifulSoup(r.text, "html.parser")
            
            # Extraire tous les titres (h1-h4) avec liens
            titles_found = 0
            for tag in soup.find_all(['h1', 'h2', 'h3', 'h4'], limit=100):
                text = tag.get_text(strip=True)
                
                # Filtrer les titres trop courts ou trop longs
                if len(text) < 15 or len(text) > 300:
                    continue
                
                # Chercher le lien associ√©
                link = ""
                link_tag = tag.find("a") or tag.find_parent("a")
                if link_tag and link_tag.get("href"):
                    href = link_tag["href"]
                    # Construire l'URL compl√®te
                    if href.startswith('http'):
                        link = href
                    elif href.startswith('/'):
                        link = urljoin(url, href)
                    else:
                        link = urljoin(url, '/' + href)
                
                # √âviter les liens internes non-articles
                if link and any(skip in link for skip in ['#', 'javascript:', 'mailto:']):
                    continue
                
                all_articles.append({
                    "source_type": "scraping",
                    "source": source,
                    "title": text,
                    "link": link,
                    "content_hash": generate_hash(text + link),
                    "retrieved_date": datetime.now().isoformat()
                })
                titles_found += 1
            
            logger.info(f"  ‚úì {titles_found} articles")
            time.sleep(2)  # Rate limiting important
            
        except requests.exceptions.Timeout:
            logger.error(f"  ‚úó Timeout pour {source}")
        except requests.exceptions.RequestException as e:
            logger.error(f"  ‚úó Erreur requ√™te: {e}")
        except Exception as e:
            logger.error(f"  ‚úó Erreur inattendue: {e}")
    
    today = datetime.today().strftime("%Y-%m-%d")
    save_data(all_articles, f"{RAW_DIR}/scraping/scraped_articles_{today}")
    logger.info(f"[Scraping] ‚úÖ Total: {len(all_articles)} articles")
    return all_articles

# --------------------------
# 6Ô∏è‚É£ Fusion et d√©duplication
# --------------------------
def combine_all_sources():
    """Combine toutes les sources et √©limine les doublons"""
    
    today = datetime.today().strftime("%Y-%m-%d")
    all_data = []
    
    logger.info("[Fusion] Chargement des donn√©es collect√©es...")
    
    # Charger tous les fichiers JSON du jour
    for subdir in ['rss', 'twitter', 'reddit', 'scraping', 'newsapi']:
        subdir_path = f"{RAW_DIR}/{subdir}"
        if not os.path.exists(subdir_path):
            continue
            
        json_files = [f for f in os.listdir(subdir_path) 
                      if f.endswith('.json') and today in f]
        
        for file in json_files:
            try:
                filepath = f"{subdir_path}/{file}"
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_data.extend(data)
                    logger.info(f"  ‚úì {file}: {len(data)} entr√©es")
            except Exception as e:
                logger.error(f"  ‚úó Erreur lecture {file}: {e}")
    
    logger.info(f"[Fusion] Total avant d√©duplication: {len(all_data)}")
    
    # D√©duplication bas√©e sur content_hash
    unique_data = {}
    duplicates = 0
    
    for item in all_data:
        hash_key = item.get('content_hash', generate_hash(str(item)))
        if hash_key not in unique_data:
            unique_data[hash_key] = item
        else:
            duplicates += 1
    
    final_data = list(unique_data.values())
    
    # Sauvegarder la version combin√©e
    save_data(final_data, f"{RAW_DIR}/combined/all_sources_{today}")
    
    logger.info(f"[Fusion] ‚úÖ {len(final_data)} articles uniques")
    logger.info(f"[Fusion] üîÑ {duplicates} doublons √©limin√©s")
    
    # Statistiques par source
    stats_by_source = {}
    for item in final_data:
        source_type = item.get('source_type', 'unknown')
        stats_by_source[source_type] = stats_by_source.get(source_type, 0) + 1
    
    logger.info("[Fusion] üìä R√©partition par source:")
    for source, count in sorted(stats_by_source.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {source}: {count} ({count/len(final_data)*100:.1f}%)")
    
    return final_data

# --------------------------
# üöÄ Ex√©cution principale
# --------------------------
if __name__ == "__main__":
    
    print("\n" + "="*80)
    print("üì∞ COLLECTEUR DE DONN√âES OPTIMIS√â - Version am√©lior√©e")
    print("="*80 + "\n")
    
    start_time = datetime.now()
    stats = {}
    
    # Phase 1: RSS (TOUJOURS actif - utilise votre feeds.json)
    print("\nüîÑ Phase 1/5: Collecte RSS (feeds.json)")
    print("-" * 80)
    try:
        rss_data = collect_rss()
        stats["rss"] = len(rss_data)
    except Exception as e:
        logger.error(f"‚ùå Erreur RSS: {e}")
        stats["rss"] = 0
    
    # Phase 2: NewsAPI (si cl√© disponible)
    print("\nüîÑ Phase 2/5: Collecte NewsAPI")
    print("-" * 80)
    try:
        newsapi_data = collect_newsapi()
        stats["newsapi"] = len(newsapi_data)
    except Exception as e:
        logger.error(f"‚ùå Erreur NewsAPI: {e}")
        stats["newsapi"] = 0
    
    # Phase 3: Twitter (si token disponible)
    print("\nüîÑ Phase 3/5: Collecte Twitter")
    print("-" * 80)
    try:
        twitter_data = collect_twitter()
        stats["twitter"] = len(twitter_data)
    except Exception as e:
        logger.error(f"‚ùå Erreur Twitter: {e}")
        stats["twitter"] = 0
    
    # Phase 4: Reddit (si identifiants disponibles)
    print("\nüîÑ Phase 4/5: Collecte Reddit")
    print("-" * 80)
    try:
        reddit_data = collect_reddit()
        stats["reddit"] = len(reddit_data)
    except Exception as e:
        logger.error(f"‚ùå Erreur Reddit: {e}")
        stats["reddit"] = 0
    
    # Phase 5: Scraping (TOUJOURS actif)
    print("\nüîÑ Phase 5/5: Web Scraping")
    print("-" * 80)
    try:
        scraping_data = collect_scraping()
        stats["scraping"] = len(scraping_data)
    except Exception as e:
        logger.error(f"‚ùå Erreur Scraping: {e}")
        stats["scraping"] = 0
    
    # Phase 6: Fusion finale
    print("\nüîÑ Phase 6: Fusion et d√©duplication")
    print("-" * 80)
    try:
        combined_data = combine_all_sources()
    except Exception as e:
        logger.error(f"‚ùå Erreur fusion: {e}")
    
    # Rapport final
    duration = (datetime.now() - start_time).total_seconds()
    total = sum(stats.values())
    
    print("\n" + "="*80)
    print("‚úÖ COLLECTE TERMIN√âE")
    print("="*80)
    print(f"‚è±Ô∏è  Dur√©e totale: {duration:.1f} secondes ({duration/60:.1f} minutes)")
    print(f"üìä Total collect√©: {total:,} √©l√©ments\n")
    print("D√©tails par source:")
    print("-" * 80)
    for source, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        print(f"   {source.upper():15s} : {count:6,} ({percentage:5.1f}%)")
    print("\n" + "="*80)
    print("‚ú® Donn√©es brutes pr√™tes pour le pr√©traitement!")
    print(f"üìÅ Emplacement: {RAW_DIR}/")
    print("="*80 + "\n")